{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import text_helpers\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a saving directory if it doesn't exist\n",
    "data_folder_name = 'temp'\n",
    "if not os.path.exists(data_folder_name):\n",
    "    os.makedirs(data_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "vocabulary_size = 7500\n",
    "generations = 100000\n",
    "model_learning_rate = 0.001\n",
    "\n",
    "embedding_size = 200   # Word embedding size\n",
    "doc_embedding_size = 100   # Document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
    "window_size = 3       # How many words to consider to the left.\n",
    "\n",
    "# Add checkpoints to training\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Normalizing Text Data\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Declare stop words\n",
    "#stops = stopwords.words('english')\n",
    "stops = []\n",
    "\n",
    "# We pick a few test words for validation.\n",
    "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']\n",
    "# Later we will have to transform these into indices\n",
    "\n",
    "# Load the movie review data\n",
    "print('Loading Data')\n",
    "texts, target = text_helpers.load_movie_data()\n",
    "\n",
    "# Normalize text\n",
    "print('Normalizing Text Data')\n",
    "texts = text_helpers.normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > window_size]\n",
    "texts = [x for x in texts if len(x.split()) > window_size]    \n",
    "assert(len(target)==len(texts))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dictionary\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Build our data set and dictionaries\n",
    "print('Creating Dictionary')\n",
    "word_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_helpers.text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(texts), doc_embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, window_size + 1]) # plus 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "# Add together element embeddings in window:\n",
    "embed = tf.zeros([batch_size, embedding_size])\n",
    "for element in range(window_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n",
    "\n",
    "doc_indices = tf.slice(x_inputs, [0,window_size],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "\n",
    "# concatenate embeddings\n",
    "final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=final_embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "                                     \n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Create model saving operation\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Loss at step 100 : 26.123146057128906\n",
      "Loss at step 200 : 21.973981857299805\n",
      "Loss at step 300 : 14.549206733703613\n",
      "Loss at step 400 : 18.90227699279785\n",
      "Loss at step 500 : 14.576988220214844\n",
      "Loss at step 600 : 16.81222152709961\n",
      "Loss at step 700 : 24.745500564575195\n",
      "Loss at step 800 : 10.66199779510498\n",
      "Loss at step 900 : 21.346149444580078\n",
      "Loss at step 1000 : 18.498979568481445\n",
      "Loss at step 1100 : 24.30424690246582\n",
      "Loss at step 1200 : 16.300081253051758\n",
      "Loss at step 1300 : 19.17396354675293\n",
      "Loss at step 1400 : 18.970230102539062\n",
      "Loss at step 1500 : 13.918855667114258\n",
      "Loss at step 1600 : 19.595441818237305\n",
      "Loss at step 1700 : 16.918054580688477\n",
      "Loss at step 1800 : 18.993946075439453\n",
      "Loss at step 1900 : 12.8231840133667\n",
      "Loss at step 2000 : 21.682323455810547\n",
      "Loss at step 2100 : 18.36751937866211\n",
      "Loss at step 2200 : 20.192577362060547\n",
      "Loss at step 2300 : 15.472284317016602\n",
      "Loss at step 2400 : 25.392480850219727\n",
      "Loss at step 2500 : 15.340847969055176\n",
      "Loss at step 2600 : 17.002729415893555\n",
      "Loss at step 2700 : 17.246822357177734\n",
      "Loss at step 2800 : 20.37779998779297\n",
      "Loss at step 2900 : 18.801050186157227\n",
      "Loss at step 3000 : 20.880525588989258\n",
      "Loss at step 3100 : 18.63888168334961\n",
      "Loss at step 3200 : 15.608113288879395\n",
      "Loss at step 3300 : 16.658124923706055\n",
      "Loss at step 3400 : 19.46840476989746\n",
      "Loss at step 3500 : 21.892738342285156\n",
      "Loss at step 3600 : 16.971115112304688\n",
      "Loss at step 3700 : 15.70079517364502\n",
      "Loss at step 3800 : 19.303308486938477\n",
      "Loss at step 3900 : 19.312889099121094\n",
      "Loss at step 4000 : 20.077472686767578\n",
      "Loss at step 4100 : 18.579063415527344\n",
      "Loss at step 4200 : 16.680307388305664\n",
      "Loss at step 4300 : 25.229764938354492\n",
      "Loss at step 4400 : 20.182973861694336\n",
      "Loss at step 4500 : 20.42719268798828\n",
      "Loss at step 4600 : 19.862422943115234\n",
      "Loss at step 4700 : 16.95149803161621\n",
      "Loss at step 4800 : 18.233861923217773\n",
      "Loss at step 4900 : 16.248376846313477\n",
      "Loss at step 5000 : 15.2088041305542\n",
      "Nearest to love: a, like, in, the, are,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, content, in, band,\n",
      "Nearest to sad: gives, adolescent, masterful, end, us,\n",
      "Nearest to man: are, enough, of, touching, accurate,\n",
      "Nearest to woman: art, scathing, what, steers, nonsense,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 5100 : 17.343914031982422\n",
      "Loss at step 5200 : 17.412649154663086\n",
      "Loss at step 5300 : 16.827280044555664\n",
      "Loss at step 5400 : 16.372600555419922\n",
      "Loss at step 5500 : 17.057300567626953\n",
      "Loss at step 5600 : 19.160118103027344\n",
      "Loss at step 5700 : 17.92497444152832\n",
      "Loss at step 5800 : 17.406469345092773\n",
      "Loss at step 5900 : 17.620134353637695\n",
      "Loss at step 6000 : 18.383230209350586\n",
      "Loss at step 6100 : 21.657527923583984\n",
      "Loss at step 6200 : 16.23993492126465\n",
      "Loss at step 6300 : 16.04213523864746\n",
      "Loss at step 6400 : 18.59441566467285\n",
      "Loss at step 6500 : 17.284631729125977\n",
      "Loss at step 6600 : 16.984399795532227\n",
      "Loss at step 6700 : 18.766040802001953\n",
      "Loss at step 6800 : 15.852431297302246\n",
      "Loss at step 6900 : 19.27203369140625\n",
      "Loss at step 7000 : 14.932402610778809\n",
      "Loss at step 7100 : 15.886384963989258\n",
      "Loss at step 7200 : 16.84911346435547\n",
      "Loss at step 7300 : 24.304845809936523\n",
      "Loss at step 7400 : 16.0150203704834\n",
      "Loss at step 7500 : 16.773624420166016\n",
      "Loss at step 7600 : 22.884906768798828\n",
      "Loss at step 7700 : 17.848691940307617\n",
      "Loss at step 7800 : 13.353790283203125\n",
      "Loss at step 7900 : 15.990838050842285\n",
      "Loss at step 8000 : 25.986106872558594\n",
      "Loss at step 8100 : 21.147287368774414\n",
      "Loss at step 8200 : 17.47405433654785\n",
      "Loss at step 8300 : 21.821744918823242\n",
      "Loss at step 8400 : 17.16800308227539\n",
      "Loss at step 8500 : 16.03536033630371\n",
      "Loss at step 8600 : 15.343825340270996\n",
      "Loss at step 8700 : 20.976736068725586\n",
      "Loss at step 8800 : 14.192248344421387\n",
      "Loss at step 8900 : 19.805212020874023\n",
      "Loss at step 9000 : 20.671480178833008\n",
      "Loss at step 9100 : 17.667097091674805\n",
      "Loss at step 9200 : 16.718856811523438\n",
      "Loss at step 9300 : 15.588932991027832\n",
      "Loss at step 9400 : 16.12760353088379\n",
      "Loss at step 9500 : 16.927875518798828\n",
      "Loss at step 9600 : 17.81974220275879\n",
      "Loss at step 9700 : 20.530221939086914\n",
      "Loss at step 9800 : 27.023855209350586\n",
      "Loss at step 9900 : 22.035463333129883\n",
      "Loss at step 10000 : 19.56939125061035\n",
      "Nearest to love: a, like, in, the, are,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, end, us,\n",
      "Nearest to man: are, enough, of, touching, accurate,\n",
      "Nearest to woman: art, what, scathing, steers, nonsense,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 10100 : 18.166156768798828\n",
      "Loss at step 10200 : 23.658180236816406\n",
      "Loss at step 10300 : 15.666886329650879\n",
      "Loss at step 10400 : 14.422706604003906\n",
      "Loss at step 10500 : 19.951129913330078\n",
      "Loss at step 10600 : 17.129371643066406\n",
      "Loss at step 10700 : 18.17593002319336\n",
      "Loss at step 10800 : 21.693857192993164\n",
      "Loss at step 10900 : 14.663475036621094\n",
      "Loss at step 11000 : 11.743123054504395\n",
      "Loss at step 11100 : 19.863815307617188\n",
      "Loss at step 11200 : 21.91252899169922\n",
      "Loss at step 11300 : 17.397859573364258\n",
      "Loss at step 11400 : 16.08356475830078\n",
      "Loss at step 11500 : 19.537757873535156\n",
      "Loss at step 11600 : 16.128311157226562\n",
      "Loss at step 11700 : 16.5145320892334\n",
      "Loss at step 11800 : 13.112576484680176\n",
      "Loss at step 11900 : 18.991960525512695\n",
      "Loss at step 12000 : 20.635740280151367\n",
      "Loss at step 12100 : 14.645259857177734\n",
      "Loss at step 12200 : 17.125261306762695\n",
      "Loss at step 12300 : 17.68193817138672\n",
      "Loss at step 12400 : 14.610204696655273\n",
      "Loss at step 12500 : 15.124802589416504\n",
      "Loss at step 12600 : 20.63994026184082\n",
      "Loss at step 12700 : 18.371736526489258\n",
      "Loss at step 12800 : 15.869222640991211\n",
      "Loss at step 12900 : 18.020002365112305\n",
      "Loss at step 13000 : 16.817277908325195\n",
      "Loss at step 13100 : 16.14081382751465\n",
      "Loss at step 13200 : 16.754098892211914\n",
      "Loss at step 13300 : 19.928531646728516\n",
      "Loss at step 13400 : 17.78459358215332\n",
      "Loss at step 13500 : 20.876344680786133\n",
      "Loss at step 13600 : 14.272287368774414\n",
      "Loss at step 13700 : 16.860668182373047\n",
      "Loss at step 13800 : 17.295230865478516\n",
      "Loss at step 13900 : 15.063400268554688\n",
      "Loss at step 14000 : 18.0753116607666\n",
      "Loss at step 14100 : 16.32578468322754\n",
      "Loss at step 14200 : 15.482585906982422\n",
      "Loss at step 14300 : 19.843772888183594\n",
      "Loss at step 14400 : 14.522745132446289\n",
      "Loss at step 14500 : 16.18143081665039\n",
      "Loss at step 14600 : 13.710371971130371\n",
      "Loss at step 14700 : 22.48881721496582\n",
      "Loss at step 14800 : 18.9060001373291\n",
      "Loss at step 14900 : 19.39402198791504\n",
      "Loss at step 15000 : 16.97438621520996\n",
      "Nearest to love: like, a, in, the, are,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, end, us,\n",
      "Nearest to man: are, enough, of, touching, accurate,\n",
      "Nearest to woman: art, what, scathing, steers, nonsense,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 15100 : 15.106084823608398\n",
      "Loss at step 15200 : 18.11186408996582\n",
      "Loss at step 15300 : 15.314820289611816\n",
      "Loss at step 15400 : 16.17618179321289\n",
      "Loss at step 15500 : 19.647977828979492\n",
      "Loss at step 15600 : 15.741188049316406\n",
      "Loss at step 15700 : 15.333110809326172\n",
      "Loss at step 15800 : 18.365570068359375\n",
      "Loss at step 15900 : 13.835772514343262\n",
      "Loss at step 16000 : 18.533714294433594\n",
      "Loss at step 16100 : 19.848926544189453\n",
      "Loss at step 16200 : 22.09221649169922\n",
      "Loss at step 16300 : 16.513654708862305\n",
      "Loss at step 16400 : 15.437546730041504\n",
      "Loss at step 16500 : 19.319406509399414\n",
      "Loss at step 16600 : 21.30915641784668\n",
      "Loss at step 16700 : 15.568001747131348\n",
      "Loss at step 16800 : 20.03951644897461\n",
      "Loss at step 16900 : 16.969696044921875\n",
      "Loss at step 17000 : 16.028417587280273\n",
      "Loss at step 17100 : 14.788804054260254\n",
      "Loss at step 17200 : 13.335716247558594\n",
      "Loss at step 17300 : 22.48526382446289\n",
      "Loss at step 17400 : 11.913320541381836\n",
      "Loss at step 17500 : 13.855618476867676\n",
      "Loss at step 17600 : 20.633949279785156\n",
      "Loss at step 17700 : 13.583147048950195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 17800 : 15.908406257629395\n",
      "Loss at step 17900 : 18.37805938720703\n",
      "Loss at step 18000 : 18.530357360839844\n",
      "Loss at step 18100 : 17.730710983276367\n",
      "Loss at step 18200 : 15.952566146850586\n",
      "Loss at step 18300 : 17.059362411499023\n",
      "Loss at step 18400 : 18.240859985351562\n",
      "Loss at step 18500 : 17.84923553466797\n",
      "Loss at step 18600 : 16.863605499267578\n",
      "Loss at step 18700 : 23.04124641418457\n",
      "Loss at step 18800 : 17.826915740966797\n",
      "Loss at step 18900 : 13.707048416137695\n",
      "Loss at step 19000 : 14.692239761352539\n",
      "Loss at step 19100 : 20.658960342407227\n",
      "Loss at step 19200 : 15.308323860168457\n",
      "Loss at step 19300 : 18.180631637573242\n",
      "Loss at step 19400 : 19.274953842163086\n",
      "Loss at step 19500 : 19.886987686157227\n",
      "Loss at step 19600 : 14.36757755279541\n",
      "Loss at step 19700 : 17.550872802734375\n",
      "Loss at step 19800 : 18.948026657104492\n",
      "Loss at step 19900 : 19.043495178222656\n",
      "Loss at step 20000 : 17.562175750732422\n",
      "Nearest to love: like, in, a, the, are,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, us, end,\n",
      "Nearest to man: are, enough, of, touching, very,\n",
      "Nearest to woman: art, what, scathing, steers, nonsense,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 20100 : 17.892526626586914\n",
      "Loss at step 20200 : 16.105567932128906\n",
      "Loss at step 20300 : 18.193159103393555\n",
      "Loss at step 20400 : 14.116559028625488\n",
      "Loss at step 20500 : 20.01349449157715\n",
      "Loss at step 20600 : 12.556168556213379\n",
      "Loss at step 20700 : 15.724424362182617\n",
      "Loss at step 20800 : 17.14719581604004\n",
      "Loss at step 20900 : 17.208101272583008\n",
      "Loss at step 21000 : 16.55585479736328\n",
      "Loss at step 21100 : 17.319276809692383\n",
      "Loss at step 21200 : 18.82540512084961\n",
      "Loss at step 21300 : 15.109667778015137\n",
      "Loss at step 21400 : 14.95926570892334\n",
      "Loss at step 21500 : 16.77448081970215\n",
      "Loss at step 21600 : 15.431347846984863\n",
      "Loss at step 21700 : 12.892594337463379\n",
      "Loss at step 21800 : 15.367300987243652\n",
      "Loss at step 21900 : 14.366878509521484\n",
      "Loss at step 22000 : 24.963851928710938\n",
      "Loss at step 22100 : 13.210350036621094\n",
      "Loss at step 22200 : 16.011734008789062\n",
      "Loss at step 22300 : 12.182734489440918\n",
      "Loss at step 22400 : 19.767656326293945\n",
      "Loss at step 22500 : 16.10038948059082\n",
      "Loss at step 22600 : 15.806973457336426\n",
      "Loss at step 22700 : 12.534453392028809\n",
      "Loss at step 22800 : 20.615684509277344\n",
      "Loss at step 22900 : 20.03372573852539\n",
      "Loss at step 23000 : 18.898874282836914\n",
      "Loss at step 23100 : 12.100589752197266\n",
      "Loss at step 23200 : 17.514917373657227\n",
      "Loss at step 23300 : 14.366988182067871\n",
      "Loss at step 23400 : 20.199922561645508\n",
      "Loss at step 23500 : 17.584516525268555\n",
      "Loss at step 23600 : 15.040194511413574\n",
      "Loss at step 23700 : 15.258593559265137\n",
      "Loss at step 23800 : 17.709308624267578\n",
      "Loss at step 23900 : 21.18403434753418\n",
      "Loss at step 24000 : 17.31915855407715\n",
      "Loss at step 24100 : 15.798795700073242\n",
      "Loss at step 24200 : 18.205217361450195\n",
      "Loss at step 24300 : 18.35650062561035\n",
      "Loss at step 24400 : 18.2135066986084\n",
      "Loss at step 24500 : 14.933693885803223\n",
      "Loss at step 24600 : 19.390073776245117\n",
      "Loss at step 24700 : 16.40982437133789\n",
      "Loss at step 24800 : 17.53239631652832\n",
      "Loss at step 24900 : 14.09696102142334\n",
      "Loss at step 25000 : 12.870108604431152\n",
      "Nearest to love: like, in, a, the, are,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, us, end,\n",
      "Nearest to man: are, enough, of, touching, very,\n",
      "Nearest to woman: art, what, scathing, steers, special,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 25100 : 17.113914489746094\n",
      "Loss at step 25200 : 18.90462303161621\n",
      "Loss at step 25300 : 16.158140182495117\n",
      "Loss at step 25400 : 13.542609214782715\n",
      "Loss at step 25500 : 11.519899368286133\n",
      "Loss at step 25600 : 15.653538703918457\n",
      "Loss at step 25700 : 12.069695472717285\n",
      "Loss at step 25800 : 19.72105598449707\n",
      "Loss at step 25900 : 13.431167602539062\n",
      "Loss at step 26000 : 16.081897735595703\n",
      "Loss at step 26100 : 21.212600708007812\n",
      "Loss at step 26200 : 17.52853012084961\n",
      "Loss at step 26300 : 18.476696014404297\n",
      "Loss at step 26400 : 15.271268844604492\n",
      "Loss at step 26500 : 15.98861312866211\n",
      "Loss at step 26600 : 20.03885269165039\n",
      "Loss at step 26700 : 13.308306694030762\n",
      "Loss at step 26800 : 18.307979583740234\n",
      "Loss at step 26900 : 14.989093780517578\n",
      "Loss at step 27000 : 17.898914337158203\n",
      "Loss at step 27100 : 19.44828987121582\n",
      "Loss at step 27200 : 16.200153350830078\n",
      "Loss at step 27300 : 18.579139709472656\n",
      "Loss at step 27400 : 13.52958869934082\n",
      "Loss at step 27500 : 15.425812721252441\n",
      "Loss at step 27600 : 14.587104797363281\n",
      "Loss at step 27700 : 16.66583251953125\n",
      "Loss at step 27800 : 16.928516387939453\n",
      "Loss at step 27900 : 17.914554595947266\n",
      "Loss at step 28000 : 14.044651985168457\n",
      "Loss at step 28100 : 12.177896499633789\n",
      "Loss at step 28200 : 17.40594482421875\n",
      "Loss at step 28300 : 20.471691131591797\n",
      "Loss at step 28400 : 16.06709098815918\n",
      "Loss at step 28500 : 12.377398490905762\n",
      "Loss at step 28600 : 16.199983596801758\n",
      "Loss at step 28700 : 18.632503509521484\n",
      "Loss at step 28800 : 14.029603958129883\n",
      "Loss at step 28900 : 15.491992950439453\n",
      "Loss at step 29000 : 16.67562484741211\n",
      "Loss at step 29100 : 27.273422241210938\n",
      "Loss at step 29200 : 12.144083976745605\n",
      "Loss at step 29300 : 17.098669052124023\n",
      "Loss at step 29400 : 15.690937042236328\n",
      "Loss at step 29500 : 16.7557430267334\n",
      "Loss at step 29600 : 14.621740341186523\n",
      "Loss at step 29700 : 16.193843841552734\n",
      "Loss at step 29800 : 18.029428482055664\n",
      "Loss at step 29900 : 14.246297836303711\n",
      "Loss at step 30000 : 17.813894271850586\n",
      "Nearest to love: like, in, a, are, the,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, us, end,\n",
      "Nearest to man: are, enough, of, touching, very,\n",
      "Nearest to woman: art, what, scathing, steers, special,\n",
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 30100 : 20.233224868774414\n",
      "Loss at step 30200 : 15.703391075134277\n",
      "Loss at step 30300 : 13.44714641571045\n",
      "Loss at step 30400 : 20.343629837036133\n",
      "Loss at step 30500 : 20.858386993408203\n",
      "Loss at step 30600 : 20.34195327758789\n",
      "Loss at step 30700 : 15.54393196105957\n",
      "Loss at step 30800 : 17.61391830444336\n",
      "Loss at step 30900 : 15.992528915405273\n",
      "Loss at step 31000 : 23.736845016479492\n",
      "Loss at step 31100 : 16.793598175048828\n",
      "Loss at step 31200 : 18.43885612487793\n",
      "Loss at step 31300 : 18.277217864990234\n",
      "Loss at step 31400 : 19.518442153930664\n",
      "Loss at step 31500 : 13.553603172302246\n",
      "Loss at step 31600 : 14.01783275604248\n",
      "Loss at step 31700 : 12.17394733428955\n",
      "Loss at step 31800 : 10.759709358215332\n",
      "Loss at step 31900 : 16.845510482788086\n",
      "Loss at step 32000 : 18.58452033996582\n",
      "Loss at step 32100 : 14.224658012390137\n",
      "Loss at step 32200 : 18.254924774169922\n",
      "Loss at step 32300 : 13.140117645263672\n",
      "Loss at step 32400 : 15.452183723449707\n",
      "Loss at step 32500 : 16.203107833862305\n",
      "Loss at step 32600 : 14.598222732543945\n",
      "Loss at step 32700 : 15.641369819641113\n",
      "Loss at step 32800 : 15.526078224182129\n",
      "Loss at step 32900 : 16.868600845336914\n",
      "Loss at step 33000 : 14.14001750946045\n",
      "Loss at step 33100 : 16.5647029876709\n",
      "Loss at step 33200 : 15.245439529418945\n",
      "Loss at step 33300 : 15.974499702453613\n",
      "Loss at step 33400 : 16.50203514099121\n",
      "Loss at step 33500 : 18.77112579345703\n",
      "Loss at step 33600 : 14.69245719909668\n",
      "Loss at step 33700 : 14.626011848449707\n",
      "Loss at step 33800 : 10.977083206176758\n",
      "Loss at step 33900 : 13.85583782196045\n",
      "Loss at step 34000 : 11.646578788757324\n",
      "Loss at step 34100 : 14.74941635131836\n",
      "Loss at step 34200 : 14.424237251281738\n",
      "Loss at step 34300 : 16.474428176879883\n",
      "Loss at step 34400 : 17.757957458496094\n",
      "Loss at step 34500 : 15.081443786621094\n",
      "Loss at step 34600 : 18.06484603881836\n",
      "Loss at step 34700 : 14.378484725952148\n",
      "Loss at step 34800 : 19.042179107666016\n",
      "Loss at step 34900 : 15.476211547851562\n",
      "Loss at step 35000 : 13.063322067260742\n",
      "Nearest to love: like, in, a, are, the,\n",
      "Nearest to hate: thick, static, descends, remarkable, crossroads,\n",
      "Nearest to happy: proportions, bumper, in, on, content,\n",
      "Nearest to sad: gives, adolescent, masterful, us, end,\n",
      "Nearest to man: are, enough, of, touching, very,\n",
      "Nearest to woman: art, what, scathing, steers, special,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: C:\\Users\\Evan\\Documents\\TensorflowExamples\\temp\\doc2vec_movie_embeddings.ckpt\n",
      "Loss at step 35100 : 16.262231826782227\n"
     ]
    }
   ],
   "source": [
    "# Run the doc2vec model.\n",
    "print('Starting Training')\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size,\n",
    "                                                                  window_size, method='doc2vec')\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    # Save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        # Save vocabulary dictionary\n",
    "        with open(os.path.join(data_folder_name,'movie_vocab.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        # Save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),data_folder_name,'doc2vec_movie_embeddings.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 20\n",
    "logistic_batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to keep the indices sorted to keep track of document index\n",
    "train_indices = np.sort(np.random.choice(len(target), round(0.8*len(target)), replace=False))\n",
    "test_indices = np.sort(np.array(list(set(range(len(target))) - set(train_indices))))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n",
    "\n",
    "# Convert texts to lists of indices\n",
    "text_data_train = np.array(text_helpers.text_to_numbers(texts_train, word_dictionary))\n",
    "text_data_test = np.array(text_helpers.text_to_numbers(texts_test, word_dictionary))\n",
    "\n",
    "# Pad/crop movie reviews to specific length\n",
    "text_data_train = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_train]])\n",
    "text_data_test = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic placeholders\n",
    "log_x_inputs = tf.placeholder(tf.int32, shape=[None, max_words + 1]) # plus 1 for doc index\n",
    "log_y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# Define logistic embedding lookup (needed if we have two different batch sizes)\n",
    "# Add together element embeddings in window:\n",
    "log_embed = tf.zeros([logistic_batch_size, embedding_size])\n",
    "for element in range(max_words):\n",
    "    log_embed += tf.nn.embedding_lookup(embeddings, log_x_inputs[:, element])\n",
    "\n",
    "log_doc_indices = tf.slice(log_x_inputs, [0,max_words],[logistic_batch_size,1])\n",
    "log_doc_embed = tf.nn.embedding_lookup(doc_embeddings,log_doc_indices)\n",
    "\n",
    "# concatenate embeddings\n",
    "log_final_embed = tf.concat(axis=1, values=[log_embed, tf.squeeze(log_doc_embed)])\n",
    "\n",
    "# Define model:\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[concatenated_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Declare logistic model (sigmoid in loss function)\n",
    "model_output = tf.add(tf.matmul(log_final_embed, A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare loss function (Cross Entropy loss)\n",
    "logistic_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=tf.cast(log_y_target, tf.float32)))\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Declare optimizer\n",
    "logistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "logistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Logistic Regression\n",
    "print('Starting Logistic Doc2Vec Model Training')\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "for i in range(10000):\n",
    "    rand_index = np.random.choice(text_data_train.shape[0], size=logistic_batch_size)\n",
    "    rand_x = text_data_train[rand_index]\n",
    "    # Append review index at the end of text data\n",
    "    rand_x_doc_indices = train_indices[rand_index]\n",
    "    rand_x = np.hstack((rand_x, np.transpose([rand_x_doc_indices])))\n",
    "    rand_y = np.transpose([target_train[rand_index]])\n",
    "    \n",
    "    feed_dict = {log_x_inputs : rand_x, log_y_target : rand_y}\n",
    "    sess.run(logistic_train_step, feed_dict=feed_dict)\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i+1)%100==0:\n",
    "        rand_index_test = np.random.choice(text_data_test.shape[0], size=logistic_batch_size)\n",
    "        rand_x_test = text_data_test[rand_index_test]\n",
    "        # Append review index at the end of text data\n",
    "        rand_x_doc_indices_test = test_indices[rand_index_test]\n",
    "        rand_x_test = np.hstack((rand_x_test, np.transpose([rand_x_doc_indices_test])))\n",
    "        rand_y_test = np.transpose([target_test[rand_index_test]])\n",
    "        \n",
    "        test_feed_dict = {log_x_inputs: rand_x_test, log_y_target: rand_y_test}\n",
    "        \n",
    "        i_data.append(i+1)\n",
    "\n",
    "        train_loss_temp = sess.run(logistic_loss, feed_dict=feed_dict)\n",
    "        train_loss.append(train_loss_temp)\n",
    "        \n",
    "        test_loss_temp = sess.run(logistic_loss, feed_dict=test_feed_dict)\n",
    "        test_loss.append(test_loss_temp)\n",
    "        \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        train_acc.append(train_acc_temp)\n",
    "    \n",
    "        test_acc_temp = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i+1)%500==0:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(i_data, train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
