{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import text_helpers\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a saving directory if it doesn't exist\n",
    "data_folder_name = 'temp'\n",
    "if not os.path.exists(data_folder_name):\n",
    "    os.makedirs(data_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "vocabulary_size = 7500\n",
    "generations = 100000\n",
    "model_learning_rate = 0.001\n",
    "\n",
    "embedding_size = 200   # Word embedding size\n",
    "doc_embedding_size = 100   # Document embedding size\n",
    "concatenated_size = embedding_size + doc_embedding_size\n",
    "\n",
    "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
    "window_size = 3       # How many words to consider to the left.\n",
    "\n",
    "# Add checkpoints to training\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Normalizing Text Data\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Declare stop words\n",
    "#stops = stopwords.words('english')\n",
    "stops = []\n",
    "\n",
    "# We pick a few test words for validation.\n",
    "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']\n",
    "# Later we will have to transform these into indices\n",
    "\n",
    "# Load the movie review data\n",
    "print('Loading Data')\n",
    "texts, target = text_helpers.load_movie_data()\n",
    "\n",
    "# Normalize text\n",
    "print('Normalizing Text Data')\n",
    "texts = text_helpers.normalize_text(texts, stops)\n",
    "\n",
    "# Texts must contain at least 3 words\n",
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > window_size]\n",
    "texts = [x for x in texts if len(x.split()) > window_size]    \n",
    "assert(len(target)==len(texts))\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dictionary\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Build our data set and dictionaries\n",
    "print('Creating Dictionary')\n",
    "word_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_helpers.text_to_numbers(texts, word_dictionary)\n",
    "\n",
    "# Get validation word keys\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Creating Model')\n",
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "doc_embeddings = tf.Variable(tf.random_uniform([len(texts), doc_embedding_size], -1.0, 1.0))\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, concatenated_size],\n",
    "                                               stddev=1.0 / np.sqrt(concatenated_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[None, window_size + 1]) # plus 1 for doc index\n",
    "y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding\n",
    "# Add together element embeddings in window:\n",
    "embed = tf.zeros([batch_size, embedding_size])\n",
    "for element in range(window_size):\n",
    "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n",
    "\n",
    "doc_indices = tf.slice(x_inputs, [0,window_size],[batch_size,1])\n",
    "doc_embed = tf.nn.embedding_lookup(doc_embeddings,doc_indices)\n",
    "\n",
    "# concatenate embeddings\n",
    "final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get loss from prediction\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=final_embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))\n",
    "                                     \n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Create model saving operation\n",
    "saver = tf.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
    "\n",
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the doc2vec model.\n",
    "print('Starting Training')\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size,\n",
    "                                                                  window_size, method='doc2vec')\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print('Loss at step {} : {}'.format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # number of nearest neighbors\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                log_str = '{} {},'.format(log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    # Save dictionary + embeddings\n",
    "    if (i+1) % save_embeddings_every == 0:\n",
    "        # Save vocabulary dictionary\n",
    "        with open(os.path.join(data_folder_name,'movie_vocab.pkl'), 'wb') as f:\n",
    "            pickle.dump(word_dictionary, f)\n",
    "        \n",
    "        # Save embeddings\n",
    "        model_checkpoint_path = os.path.join(os.getcwd(),data_folder_name,'doc2vec_movie_embeddings.ckpt')\n",
    "        save_path = saver.save(sess, model_checkpoint_path)\n",
    "        print('Model saved in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 20\n",
    "logistic_batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to keep the indices sorted to keep track of document index\n",
    "train_indices = np.sort(np.random.choice(len(target), round(0.8*len(target)), replace=False))\n",
    "test_indices = np.sort(np.array(list(set(range(len(target))) - set(train_indices))))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])\n",
    "\n",
    "# Convert texts to lists of indices\n",
    "text_data_train = np.array(text_helpers.text_to_numbers(texts_train, word_dictionary))\n",
    "text_data_test = np.array(text_helpers.text_to_numbers(texts_test, word_dictionary))\n",
    "\n",
    "# Pad/crop movie reviews to specific length\n",
    "text_data_train = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_train]])\n",
    "text_data_test = np.array([x[0:max_words] for x in [y+[0]*max_words for y in text_data_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Logistic placeholders\n",
    "log_x_inputs = tf.placeholder(tf.int32, shape=[None, max_words + 1]) # plus 1 for doc index\n",
    "log_y_target = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# Define logistic embedding lookup (needed if we have two different batch sizes)\n",
    "# Add together element embeddings in window:\n",
    "log_embed = tf.zeros([logistic_batch_size, embedding_size])\n",
    "for element in range(max_words):\n",
    "    log_embed += tf.nn.embedding_lookup(embeddings, log_x_inputs[:, element])\n",
    "\n",
    "log_doc_indices = tf.slice(log_x_inputs, [0,max_words],[logistic_batch_size,1])\n",
    "log_doc_embed = tf.nn.embedding_lookup(doc_embeddings,log_doc_indices)\n",
    "\n",
    "# concatenate embeddings\n",
    "log_final_embed = tf.concat(axis=1, values=[log_embed, tf.squeeze(log_doc_embed)])\n",
    "\n",
    "# Define model:\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[concatenated_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Declare logistic model (sigmoid in loss function)\n",
    "model_output = tf.add(tf.matmul(log_final_embed, A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare loss function (Cross Entropy loss)\n",
    "logistic_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=tf.cast(log_y_target, tf.float32)))\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, tf.cast(log_y_target, tf.float32)), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Declare optimizer\n",
    "logistic_opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "logistic_train_step = logistic_opt.minimize(logistic_loss, var_list=[A, b])\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Logistic Regression\n",
    "print('Starting Logistic Doc2Vec Model Training')\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "for i in range(10000):\n",
    "    rand_index = np.random.choice(text_data_train.shape[0], size=logistic_batch_size)\n",
    "    rand_x = text_data_train[rand_index]\n",
    "    # Append review index at the end of text data\n",
    "    rand_x_doc_indices = train_indices[rand_index]\n",
    "    rand_x = np.hstack((rand_x, np.transpose([rand_x_doc_indices])))\n",
    "    rand_y = np.transpose([target_train[rand_index]])\n",
    "    \n",
    "    feed_dict = {log_x_inputs : rand_x, log_y_target : rand_y}\n",
    "    sess.run(logistic_train_step, feed_dict=feed_dict)\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i+1)%100==0:\n",
    "        rand_index_test = np.random.choice(text_data_test.shape[0], size=logistic_batch_size)\n",
    "        rand_x_test = text_data_test[rand_index_test]\n",
    "        # Append review index at the end of text data\n",
    "        rand_x_doc_indices_test = test_indices[rand_index_test]\n",
    "        rand_x_test = np.hstack((rand_x_test, np.transpose([rand_x_doc_indices_test])))\n",
    "        rand_y_test = np.transpose([target_test[rand_index_test]])\n",
    "        \n",
    "        test_feed_dict = {log_x_inputs: rand_x_test, log_y_target: rand_y_test}\n",
    "        \n",
    "        i_data.append(i+1)\n",
    "\n",
    "        train_loss_temp = sess.run(logistic_loss, feed_dict=feed_dict)\n",
    "        train_loss.append(train_loss_temp)\n",
    "        \n",
    "        test_loss_temp = sess.run(logistic_loss, feed_dict=test_feed_dict)\n",
    "        test_loss.append(test_loss_temp)\n",
    "        \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict=feed_dict)\n",
    "        train_acc.append(train_acc_temp)\n",
    "    \n",
    "        test_acc_temp = sess.run(accuracy, feed_dict=test_feed_dict)\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i+1)%500==0:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(i_data, train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
