{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "embedding_size = 64 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state1 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  saved_output2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state2 = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "    \n",
    "  # Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "  m_rows = 4\n",
    "  m_input_index = 0\n",
    "  m_forget_index = 1\n",
    "  m_update_index = 2\n",
    "  m_output_index = 3\n",
    "  m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "   \n",
    "  \n",
    "  # Definition of the 2nd LSTM layer\n",
    "  m_input_w2 = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  m_middle_w2 = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  m_biases2 = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "  m_saved_output2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  m_input2 = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "# Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    m_input = tf.stack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.stack([o for _ in range(m_rows)])\n",
    "    m_all = tf.matmul(m_input, m_input_w) + tf.matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unstack(m_all)\n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  def lstm_cell1(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"    \n",
    "    m_input2 = tf.stack([i for _ in range(m_rows)])\n",
    "    m_saved_output2 = tf.stack([o for _ in range(m_rows)])\n",
    "    \n",
    "   # m_input2 = tf.nn.dropout(m_input2, keep_prob)\n",
    "    m_all = tf.matmul(m_input2, m_input_w2) + tf.matmul(m_saved_output2, m_middle_w2) + m_biases\n",
    "    m_all = tf.unstack(m_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by twos time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output1 = saved_output1\n",
    "  state1 = saved_state1\n",
    "  output2 = saved_output2\n",
    "  state2 = saved_state2\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output1, state1 = lstm_cell(i_embed, output1, state1)\n",
    "    output2, state2 = lstm_cell1(output1, output2, state2)\n",
    "    outputs.append(output2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output1.assign(output1),\n",
    "                                saved_state1.assign(state1),\n",
    "                                saved_output2.assign(output2),\n",
    "                                saved_state2.assign(state2)]):\n",
    "    # Classifier.\n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits,labels=tf.concat(train_labels,0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):  \n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  \n",
    "  saved_sample_output1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state2 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state1.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output2.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state2.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output1, sample_state1 = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output1, saved_sample_state1)\n",
    "  sample_output2, sample_state2 = lstm_cell1(\n",
    "    sample_output1, saved_sample_output2, saved_sample_state2)\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output1.assign(sample_output1),\n",
    "                                saved_sample_state1.assign(sample_state1),\n",
    "                                saved_sample_output2.assign(sample_output2),\n",
    "                                saved_sample_state2.assign(sample_state2)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output2, w, b))\n",
    "\n",
    "\n",
    "\n",
    "import collections\n",
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-fe61f22b65ad>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.297309 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "fajpkkli zpgosbdb liaamjdz n cdqcpaqvxq  ukgkv szayytblcpel q zt gm fyaieimcxgidj\n",
      "nlzcytxlodpxhr woozpwf u lusuxd gizw atoyye nm vaal t lokb t jbdfi tobs  tzw wawt\n",
      "bmdiua chledcoqorguhfdqcig hq jrno mcjiy bmdetjf zl mu mfeakagaae etmdiq ehfnjaed\n",
      "lxizlonyonsjh rtyorrlx o iijxade ojbaj to  hoeh rqsjhnthemozsm ubm lxrkrintnmasmb\n",
      "dwmip odqhlza mvvglfny bldw jsxqleztshpz irj hskls i tniacvyjpe oe nnr dglyiir  n\n",
      "================================================================================\n",
      "Validation set perplexity: 19.89\n",
      "Average loss at step 100: 2.690643 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.11\n",
      "Validation set perplexity: 11.76\n",
      "Average loss at step 200: 2.212783 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.39\n",
      "Validation set perplexity: 9.45\n",
      "Average loss at step 300: 2.025506 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 400: 1.909781 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 500: 1.836962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 600: 1.817781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 700: 1.763643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 800: 1.718548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 900: 1.723664 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1000: 1.732639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "uyementary chintroad in anyual the system of mach wears art hale sarian a contric\n",
      "tqas sam pe succes of see from quisk the very is supes head comporis owcte at emp\n",
      "hss dost upposlated of succusdoglatanjanian to voim usecompreet of the ceevid grd\n",
      "ihing a alnliy the sp proce as of a of gintribe for a servireduce work from ww pr\n",
      "jw in a numnettakesii espolitial cometic solutionia s an mess existing samual equ\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1100: 1.686175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 1200: 1.669669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 1300: 1.648286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 1400: 1.661670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1500: 1.652039 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1600: 1.664723 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 1700: 1.635821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 1800: 1.605548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 1900: 1.578435 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 2000: 1.626885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "mb and from this s sumility aptitous to systors to joh which and ax by pians mala\n",
      "ptionary lends moreag multuris world doss instities operafard thension john respo\n",
      "algebrateebting fightly whoz comical baskes on henchesuk old only huals being out\n",
      "dxo trans also but extendent and butning of compedice china by the huntificate st\n",
      "llist quart namer and as one nine zero zero eight a varary niven textly enguaeone\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 2100: 1.612650 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 2200: 1.606786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 2300: 1.573391 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 2400: 1.596296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 2500: 1.618868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 2600: 1.590963 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 2700: 1.605509 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 2800: 1.599403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 2900: 1.598276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 3000: 1.597801 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "jyed it six nine five uses ca cros formew dublingediatic themsku numeraula result\n",
      "gvders japars this exing church and was direct distestant indip yellevation in ci\n",
      "ds s his yeon underscience well to hacny germedian in one jight their one n now w\n",
      "vfice jotal liberonge an appsix his veory a importancizinio s conseding gambilati\n",
      "y landersair vudiang v and american kinecled most frenchis lawc however ree pub a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 3100: 1.575932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 3200: 1.603004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 3300: 1.585633 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 3400: 1.614102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 3500: 1.614280 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 3600: 1.626378 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 3700: 1.598621 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 3800: 1.605707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 3900: 1.592787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 4000: 1.605103 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "lk the is developenotal two reconoming amentrles of blue have to jember of betwee\n",
      "ey y commen dempir eyed in madanian of air use the kered had but contreat one bru\n",
      " los are novel it masist mashika with between one year is miniquid direct evor di\n",
      " aming the prince by santes king j differencism lititutates overder provided orig\n",
      " hasmanthlan external years to only the hod presendance and beakassians reguld so\n",
      "================================================================================\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 4100: 1.595139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 4200: 1.598122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.27\n",
      "Average loss at step 4300: 1.574171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 4400: 1.571807 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 4500: 1.583601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 4600: 1.575859 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 4700: 1.588667 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 4800: 1.592634 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 4900: 1.601017 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 5000: 1.572136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "me such one nine nine six three origins high rised breake it israudames sily libe\n",
      "iylavim slavon minist down of lements from the rangis director ill the i seement \n",
      "zwill mograystus beach of can be desian linky one four two banage bown of neug si\n",
      "aq one zero do sle hit ra one zero zero rediations is a banymonsgang veek bated w\n",
      "wqnation were one relation discupited led perbern but prousen they intelline disc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 5100: 1.578113 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 5200: 1.551980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 5300: 1.535728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5400: 1.537979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 5500: 1.523439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 5600: 1.537274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 5700: 1.525935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 5800: 1.537085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 5900: 1.529075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 6000: 1.498094 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "fxcal processed the flagbses was in are tevtors of subviie asee they a prome case\n",
      "cnstant out of imperial with accords cously holowed in heistical one five two his\n",
      " your exten a powerefore europm referon one there are any namerically feutical ci\n",
      "der of a not harriasinates the vicoale six interaditional strepvged however inter\n",
      "pner to viefevel and being test resigned piller is these st metal riike by coin t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 6100: 1.522260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 6200: 1.484046 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 6300: 1.495384 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 6400: 1.493248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 6500: 1.505633 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 6600: 1.548182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 6700: 1.529389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6800: 1.547712 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 6900: 1.525791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 7000: 1.522974 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "wvariation reep of board more recodess are girlinicature politics contraspulars a\n",
      "pickly be groups on these edy hcentnally they d final raperty to be order with th\n",
      "xdnub official systems walk atlanded the footract order of the soline by general \n",
      "pa flealne the so beinworks purposes historics on about pernment who and for trie\n",
      "ajent bindu and legal raelge in the windoland to one four meld conception with th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 7100: 1.523121 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 7200: 1.522088 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 7300: 1.517441 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 7400: 1.525225 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 7500: 1.532656 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 7600: 1.503974 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 7700: 1.498530 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 7800: 1.523609 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 7900: 1.527785 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 8000: 1.561167 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "kground moles delavovidobt underrison and purpoting to susmanuary change translat\n",
      "zgs ao planetin in a the uses and amaz the plottle attempt leagues that all const\n",
      "lving has now to data and mada since which iv from one nine six six early in the \n",
      "fepauherly english circular one five exprestion from separating nique hose one ol\n",
      "tz that of are telebinge of mainent of unrochier sode haypical been symlowed the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 8100: 1.542298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 8200: 1.513543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 8300: 1.508734 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 8400: 1.523682 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 8500: 1.521274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 8600: 1.522683 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 8700: 1.512375 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 8800: 1.488772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 8900: 1.502355 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.94\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 9000: 1.500167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "critic turthout many in fir wire rebell ky is india up a memethrre hat s good so \n",
      "cillogy as oppomement golde make in one nine nine of it been  unamericant and pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vill el dulaneri v d one nine seven zero totation of reign a ben british create p\n",
      "rd encygy a new system of reducation howers well male of his continent greated ma\n",
      "yk parties the inside of also were rebelled by logrey past or islamed the israel \n",
      "================================================================================\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 9100: 1.506918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 9200: 1.531092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 9300: 1.543072 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 9400: 1.529419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 9500: 1.532704 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 9600: 1.513419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 9700: 1.532731 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 9800: 1.538749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 9900: 1.529024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 10000: 1.550506 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "ctive can during preccactive via one eight zero birth herists of the significatio\n",
      "wver frontration into clair of the lordhab compression in a gream infare edition \n",
      "bases unspos takka talexts octural collection as mainery so has the army the lriu\n",
      "gges oblh them sequences and aller of the name that he has in the moneys youths c\n",
      "eovariancymcdo poittan are chcathound and a moletic idensitleotal boseal commeste\n",
      "================================================================================\n",
      "Validation set perplexity: 5.81\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
